# -*- coding: utf-8 -*-
"""Save weights of Cleaned Job interview STARE classification

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Szat3WgiTOHb0KO4DR2wxEEp0_ZtDW_d

## Import libary
"""

# basic library
import pandas as pd
import numpy as np
import os
import csv
import matplotlib.pyplot as plt
import seaborn as sns
import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer
import re, string

# TensorFlow library
import tensorflow_hub as hub
import tensorflow as tf
import keras
from keras import backend as K
from keras.models import save_model

# scikit-learn library
import sklearn
from sklearn import metrics
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, multilabel_confusion_matrix
from sklearn.metrics import classification_report
from sklearn.metrics import auc, roc_curve
from sklearn.metrics import roc_auc_score
from sklearn.metrics import precision_recall_curve, average_precision_score

print(tf.__version__)

from google.colab import drive
drive.mount('/content/drive')

path_Q1 = '/content/drive/My Drive/Floww/Q1.csv'
path_Q3 = '/content/drive/My Drive/Floww/Q3.csv'
path_more = '/content/drive/My Drive/Floww/more.csv'
question_Q1 = pd.read_csv(path_Q1)
question_Q3 = pd.read_csv(path_Q3)
question_more = pd.read_csv(path_more)

question = pd.concat([question_Q1, question_Q3, question_more], ignore_index=True, sort=False)

"""## Data overview"""

question.shape

question[['S', 'T', 'A', 'R', 'E']].sum(axis=0)

assert question.Answer_In_Sentence.isna().sum != 0

question = question.fillna(0)

question = question.iloc[:, 2:-1]

question.head(3)

convert_dict = {'S':int,'T':int,'A':int,'R':int,'E':int}

question = question.astype(convert_dict)

question.head(3)

question.tail(3)

question = question.drop_duplicates()

question.shape

labels = np.array(question[['S', 'T', 'A', 'R', 'E']], dtype=np.int8)

labels[:3]

"""## Clean data for preparation"""

def clean_text(text_df):
  cleaned = []
  for line in text_df:
    # tokenize
    tokens = word_tokenize(line)
    # lowercase
    lowercases = [word.lower() for word in tokens]
    # remove digits, etc
    regularwords = [word for word in lowercases if word.isalpha()]
    # remove punctuation
    punc = str.maketrans('', '', string.punctuation)
    stripped = [word.translate(punc) for word in regularwords]
    # remove stopwords
    stop_words = set(stopwords.words('english'))
    cleanwords = [word for word in stripped if not word in stop_words]
    # stemming
    # porter = PorterStemmer()
    # stemmed = [porter.stem(word) for word in cleanwords]
    sentence = [' '.join(cleanwords)]
    cleaned.append(sentence)
  return cleaned

cleaned_texts = clean_text(question['Answer_In_Sentence'])

cleaned_texts = np.array(cleaned_texts)

cleaned_texts[:3]

texts[:3]

# train/val = 8/2 ratio
train_text, val_text, train_label, val_label = train_test_split(cleaned_texts, labels, test_size=0.2, random_state=42, shuffle=True)

print(train_text.shape, val_text.shape, val_label.shape)

"""### Base model - Universal sentence encoder

reference: https://www.dlology.com/blog/keras-meets-universal-sentence-encoder-transfer-learning-for-text-data/
"""

module_url = "https://tfhub.dev/google/universal-sentence-encoder-large/3"

# Import the Universal Sentence Encoder's TF Hub module
embed = hub.Module(module_url)

embed_size = 512

# Removes a 1-dimension from the tensor at index "axis".
def UniversalEmbedding(x):
  return embed(tf.squeeze(tf.cast(x, tf.string), axis=1), signature='default', as_dict=True)['default']

input_text = keras.layers.Input(shape=(1,), dtype=tf.string)
embedding = keras.layers.Lambda(UniversalEmbedding, output_shape=(embed_size,))(input_text)
dense = keras.layers.Dense(128, activation='relu')(embedding)
pred = keras.layers.Dense(5, activation='sigmoid')(dense)
model = keras.Model(inputs=[input_text], outputs=pred)
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
model.summary()

sess = tf.Session()
K.set_session(sess)
sess.run(tf.global_variables_initializer())
sess.run(tf.tables_initializer())
history = model.fit(train_text, train_label, validation_data=(val_text, val_label), epochs = 30, batch_size=16)
print('\ntest accuracy on entire dataset')
print(model.evaluate(cleaned_texts, labels))
y_preds = model.predict(cleaned_texts)
# -- weights are fine --
print('\nsave weights of the model')
model.save_weights('./ModelWeights.h5')

import matplotlib.pyplot as plt
def plot_graphs(history, string):
  plt.plot(history.history[string])
  plt.plot(history.history['val_'+string])
  plt.xlabel('Epochs')
  plt.ylabel(string)
  plt.legend([string, 'val_'+string])
  plt.show()
  
plot_graphs(history, 'acc')
plot_graphs(history, 'loss')

y_preds[:5]

y_pred = (y_preds > 0.5)

print(metrics.classification_report(labels, y_pred))

roc_auc_score(labels, y_preds)

average_precision_score(labels, y_preds)

class_names = ['S', 'T', 'A', 'R', 'E']

fpr = dict()
tpr = dict()
roc_auc = dict()
n_classes = 5
for i in range(n_classes):
    fpr[i], tpr[i], _ = roc_curve(labels[:, i], y_preds[:, i])
    roc_auc[i] = auc(fpr[i], tpr[i])
for i in range(n_classes):
    plt.plot(fpr[i], tpr[i],
             label='ROC curve of class {0} (area = {1:0.2f})'
             ''.format(class_names[i], roc_auc[i]))

plt.plot([0, 1], [0, 1], 'k--')
plt.xlim([-0.05, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver operating characteristic for multi-class data')
plt.legend(loc="lower right")
plt.show()

precision = dict()
recall = dict()
average_precision = dict()
for i in range(n_classes):
    precision[i], recall[i], _ = precision_recall_curve(labels[:, i], y_preds[:, i])
    average_precision[i] = average_precision_score(labels[:, i], y_preds[:, i])
for i in range(n_classes):
    plt.plot(precision[i], recall[i],
             label='ROC curve of class {0} (area = {1:0.2f})'
             ''.format(class_names[i], average_precision[i]))

plt.plot([0, 1], [0, 1], 'k--')
plt.xlim([-0.05, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('PR AUC for multi-class data')
plt.legend(loc="lower right")
plt.show()

y_pred_new = np.where(y_preds > 0.5, 1, 0)

cm = multilabel_confusion_matrix(labels, y_pred_new)

cm

# what if each text only have one category
def plot_confusion_matrix(true_y, pred_y, category_names):
  cm = confusion_matrix(true_y.argmax(axis=1), pred_y.argmax(axis=1))
  cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
  df_cm = pd.DataFrame(cm, columns=class_names, index=class_names)
  df_cm.index.name = 'Actual'
  df_cm.columns.name = 'Predicted'
  plt.figure(figsize = (8,5))
  sns.set(font_scale=1.5)
  sns.heatmap(df_cm, cmap='Blues', annot=True, annot_kws={"size": 16})

plot_confusion_matrix(labels, y_preds, class_names)

"""### Use weights to predict new dataset"""

next_text = ['I feel proud of myself after this experience.',
             'Some days ago I met an angry customer who want to return his item.',
             'I am a cashier in a fashion shop.',
             'He finally calmed down and went away.',
             'He is a good co-worker of me.',
             'Thank godness.']

new_text = np.array(clean_text(next_text))

with tf.Session() as session:
  K.set_session(session)
  session.run(tf.global_variables_initializer())
  session.run(tf.tables_initializer())
  model.load_weights('./ModelWeights.h5')
  predicts = model.predict(new_text, batch_size=1)
predict_new = np.where(predicts > 0.5, 1, 0)

predict_new

"""### Use saved model to predict new model"""

from keras.models import load_model

trained_model = load_model('SavedModel.h5')

prediction_new = trained_model.predict(new_text)