# Gradient descent

This project takes a dataset that has 16 features and try to build a multivariant linear regression by:

* Direct solution by solving the formula

* Using cost function:

* Find solution by gradient descent

* Find solution by gradient descent with regulizer
  
* Find solution by mini-batch gradient descent: use the only batch weights to update the model weights

## Objective

  - Explore math theory inside gradient descent and use linear regression here as an example.
  - See difference between *gradient descent* and *mini-batch* and how *learning rates*, *batch size*, *regulization* have impact on converge times and train/test error.

## Good points:
  - Vectorization
  - Deep understanding in mathematics
  - How to choose hyparameters
 
