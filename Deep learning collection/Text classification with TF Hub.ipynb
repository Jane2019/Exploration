{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text classification with TF-Hub",
      "provenance": [],
      "collapsed_sections": [
        "N6ZDpd9XzFeN"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "N6ZDpd9XzFeN"
      },
      "source": [
        "##### Copyright 2018 The TensorFlow Hub Authors.\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "colab_type": "code",
        "id": "KUu4vOt5zI9d",
        "colab": {}
      },
      "source": [
        "# Copyright 2018 The TensorFlow Hub Authors. All Rights Reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "# =============================================================================="
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ok9PfyoQ2rH_"
      },
      "source": [
        "# How to build a simple text classifier with TF-Hub\n",
        "\n",
        "<table align=\"left\"><td>\n",
        "  <a target=\"_blank\"  href=\"https://colab.research.google.com/github/tensorflow/hub/blob/master/docs/tutorials/text_classification_with_tf_hub.ipynb\">\n",
        "    <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab\n",
        "  </a>\n",
        "</td><td>\n",
        "  <a target=\"_blank\"  href=\"https://github.com/tensorflow/hub/blob/master/docs/tutorials/text_classification_with_tf_hub.ipynb\">\n",
        "    <img width=32px src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "</td></table>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "AK3mz3JNMW8Y"
      },
      "source": [
        "TF-Hub is a platform to share machine learning expertise packaged in reusable resources, notably pre-trained **modules**. This tutorial is organized into two main parts.\n",
        "\n",
        "** *Introduction:* Training a text classifier with TF-Hub**\n",
        "\n",
        "We will use a TF-Hub text embedding module to train a simple sentiment classifier with a reasonable baseline accuracy. We will then analyze the predictions to make sure our model is reasonable and propose improvements to increase the accuracy.\n",
        "\n",
        "** *Advanced:* Transfer learning analysis **\n",
        "\n",
        "In this section, we will use various TF-Hub modules to compare their effect on the accuracy of the estimator and demonstrate advantages and pitfalls of transfer learning.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "aYVd26q1_3xW"
      },
      "source": [
        "## Optional prerequisites\n",
        "\n",
        "* Basic understanding of Tensorflow [premade estimator framework](https://www.tensorflow.org/get_started/premade_estimators).\n",
        "* Familiarity with [Pandas](https://pandas.pydata.org/) library.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xOATihhH1IxS"
      },
      "source": [
        "## Preparing the environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "code",
        "colab_type": "code",
        "id": "_8N3Hx2dyUC-",
        "outputId": "4698f25e-4ebe-4085-8982-e3fda7dda25a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 331
        }
      },
      "source": [
        "# Install TF-Hub.\n",
        "!pip install tensorflow-hub\n",
        "!pip install seaborn"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow-hub in /usr/local/lib/python3.6/dist-packages (0.6.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-hub) (1.12.0)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-hub) (1.16.5)\n",
            "Requirement already satisfied: protobuf>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-hub) (3.7.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.4.0->tensorflow-hub) (41.2.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.6/dist-packages (0.9.0)\n",
            "Requirement already satisfied: pandas>=0.15.2 in /usr/local/lib/python3.6/dist-packages (from seaborn) (0.24.2)\n",
            "Requirement already satisfied: numpy>=1.9.3 in /usr/local/lib/python3.6/dist-packages (from seaborn) (1.16.5)\n",
            "Requirement already satisfied: matplotlib>=1.4.3 in /usr/local/lib/python3.6/dist-packages (from seaborn) (3.0.3)\n",
            "Requirement already satisfied: scipy>=0.14.0 in /usr/local/lib/python3.6/dist-packages (from seaborn) (1.3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.5.0 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.15.2->seaborn) (2.5.3)\n",
            "Requirement already satisfied: pytz>=2011k in /usr/local/lib/python3.6/dist-packages (from pandas>=0.15.2->seaborn) (2018.9)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1.4.3->seaborn) (1.1.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1.4.3->seaborn) (2.4.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1.4.3->seaborn) (0.10.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.5.0->pandas>=0.15.2->seaborn) (1.12.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib>=1.4.3->seaborn) (41.2.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "tRXN9a8Mz8e-"
      },
      "source": [
        "More detailed information about installing Tensorflow can be found at [https://www.tensorflow.org/install/](https://www.tensorflow.org/install/)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "v7hy0bhngTUp",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import re\n",
        "import seaborn as sns"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6OPyVxHuiTEE"
      },
      "source": [
        "# Getting started\n",
        "\n",
        "## Data\n",
        "We will try to solve the [Large Movie Review Dataset v1.0](http://ai.stanford.edu/~amaas/data/sentiment/) task from Mass et al. The dataset consists of IMDB movie reviews labeled by positivity from 1 to 10. The task is to label the reviews as **negative** or **positive**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "both",
        "colab_type": "code",
        "id": "rKzc-fOGV72G",
        "outputId": "31ba62ec-4031-45b9-ae3b-cf194514a364",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        }
      },
      "source": [
        "# Load all files from a directory in a DataFrame.\n",
        "def load_directory_data(directory):\n",
        "  data = {}\n",
        "  data[\"sentence\"] = []\n",
        "  data[\"sentiment\"] = []\n",
        "  for file_path in os.listdir(directory):\n",
        "    with tf.gfile.GFile(os.path.join(directory, file_path), \"r\") as f:\n",
        "      data[\"sentence\"].append(f.read())\n",
        "      data[\"sentiment\"].append(re.match(\"\\d+_(\\d+)\\.txt\", file_path).group(1))\n",
        "  return pd.DataFrame.from_dict(data)\n",
        "\n",
        "# Merge positive and negative examples, add a polarity column and shuffle.\n",
        "def load_dataset(directory):\n",
        "  pos_df = load_directory_data(os.path.join(directory, \"pos\"))\n",
        "  neg_df = load_directory_data(os.path.join(directory, \"neg\"))\n",
        "  pos_df[\"polarity\"] = 1\n",
        "  neg_df[\"polarity\"] = 0\n",
        "  return pd.concat([pos_df, neg_df]).sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "# Download and process the dataset files.\n",
        "def download_and_load_datasets(force_download=False):\n",
        "  dataset = tf.keras.utils.get_file(\n",
        "      fname=\"aclImdb.tar.gz\", \n",
        "      origin=\"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\", \n",
        "      extract=True)\n",
        "  \n",
        "  train_df = load_dataset(os.path.join(os.path.dirname(dataset), \n",
        "                                       \"aclImdb\", \"train\"))\n",
        "  test_df = load_dataset(os.path.join(os.path.dirname(dataset), \n",
        "                                      \"aclImdb\", \"test\"))\n",
        "  \n",
        "  return train_df, test_df\n",
        "\n",
        "# Reduce logging output.\n",
        "tf.logging.set_verbosity(tf.logging.ERROR)\n",
        "\n",
        "train_df, test_df = download_and_load_datasets()\n",
        "train_df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
            "84131840/84125825 [==============================] - 9s 0us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>polarity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Charlotte's deadly beauty and lethal kicks mak...</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Please Don't hate me but i have to be honest, ...</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>In 17th Century Japan, there lived a samurai w...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Boring and appallingly acted(Summer Pheonix). ...</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>I just finished watching Going Overboard. I ha...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            sentence sentiment  polarity\n",
              "0  Charlotte's deadly beauty and lethal kicks mak...         2         0\n",
              "1  Please Don't hate me but i have to be honest, ...         3         0\n",
              "2  In 17th Century Japan, there lived a samurai w...         1         0\n",
              "3  Boring and appallingly acted(Summer Pheonix). ...         4         0\n",
              "4  I just finished watching Going Overboard. I ha...         1         0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "D9Xq4x1mU3un"
      },
      "source": [
        "## Model\n",
        "### Input functions\n",
        "\n",
        "[Estimator framework](https://www.tensorflow.org/get_started/premade_estimators#overview_of_programming_with_estimators) provides [input functions](https://www.tensorflow.org/api_docs/python/tf/estimator/inputs/pandas_input_fn) that wrap Pandas dataframes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "both",
        "colab_type": "code",
        "id": "25rdoEHih0fm",
        "colab": {}
      },
      "source": [
        "# Training input on the whole training set with no limit on training epochs.\n",
        "train_input_fn = tf.estimator.inputs.pandas_input_fn(\n",
        "    train_df, train_df[\"polarity\"], num_epochs=None, shuffle=True)\n",
        "\n",
        "# Prediction on the whole training set.\n",
        "predict_train_input_fn = tf.estimator.inputs.pandas_input_fn(\n",
        "    train_df, train_df[\"polarity\"], shuffle=False)\n",
        "# Prediction on the test set.\n",
        "predict_test_input_fn = tf.estimator.inputs.pandas_input_fn(\n",
        "    test_df, test_df[\"polarity\"], shuffle=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Uyl6YGRcVAwP"
      },
      "source": [
        "### Feature columns\n",
        "\n",
        "TF-Hub provides a [feature column](https://github.com/tensorflow/hub/blob/master/docs/api_docs/python/hub/text_embedding_column.md) that applies a module on the given text feature and passes further the outputs of the module. In this tutorial we will be using the [nnlm-en-dim128 module](https://tfhub.dev/google/nnlm-en-dim128/1). For the purpose of this tutorial, the most important facts are:\n",
        "\n",
        "* The module takes **a batch of sentences in a 1-D tensor of strings** as input.\n",
        "* The module is responsible for **preprocessing of sentences** (e.g. removal of punctuation and splitting on spaces).\n",
        "* The module works with any input (e.g. **nnlm-en-dim128** hashes words not present in vocabulary into ~20.000 buckets)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "X7vyvj-hDEXu",
        "colab": {}
      },
      "source": [
        "embedded_text_feature_column = hub.text_embedding_column(\n",
        "    key=\"sentence\", \n",
        "    module_spec=\"https://tfhub.dev/google/nnlm-en-dim128/1\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YPuHgx3BWBOg"
      },
      "source": [
        "### Estimator\n",
        "\n",
        "For classification we can use a [DNN Classifier](https://www.tensorflow.org/api_docs/python/tf/estimator/DNNClassifier) (note further remarks about different modelling of the label function at the end of the tutorial)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "23U30yEkVq4w",
        "colab": {}
      },
      "source": [
        "estimator = tf.estimator.DNNClassifier(\n",
        "    hidden_units=[500, 100],\n",
        "    feature_columns=[embedded_text_feature_column],\n",
        "    n_classes=2,\n",
        "    optimizer=tf.train.AdagradOptimizer(learning_rate=0.003))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-O_k-8jgWPXY"
      },
      "source": [
        "### Training\n",
        "\n",
        "Train the estimator for a reasonable amount of steps."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "e5uDRv1r7Ed4",
        "colab": {}
      },
      "source": [
        "# Training for 1,000 steps means 128,000 training examples with the default\n",
        "# batch size. This is roughly equivalent to 5 epochs since the training dataset\n",
        "# contains 25,000 examples.\n",
        "estimator.train(input_fn=train_input_fn, steps=1000);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "s8j7YTRSe7Pj"
      },
      "source": [
        "# Prediction\n",
        "\n",
        "Run predictions for both training and test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zbLg5LzGwAfC",
        "outputId": "aa524742-2a65-4263-d52f-d2504c0aa8f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "train_eval_result = estimator.evaluate(input_fn=predict_train_input_fn)\n",
        "test_eval_result = estimator.evaluate(input_fn=predict_test_input_fn)\n",
        "\n",
        "print(\"Training set accuracy: {accuracy}\".format(**train_eval_result))\n",
        "print(\"Test set accuracy: {accuracy}\".format(**test_eval_result))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training set accuracy: 0.8022800087928772\n",
            "Test set accuracy: 0.7946799993515015\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DR2IsTF5vuAX"
      },
      "source": [
        "## Confusion matrix\n",
        "\n",
        "We can visually check the confusion matrix to understand the distribution of misclassifications."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nT71CtArpsKz",
        "outputId": "0a41cda0-6a9c-4827-d1a7-3597c9440fae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        }
      },
      "source": [
        "def get_predictions(estimator, input_fn):\n",
        "  return [x[\"class_ids\"][0] for x in estimator.predict(input_fn=input_fn)]\n",
        "\n",
        "LABELS = [\n",
        "    \"negative\", \"positive\"\n",
        "]\n",
        "\n",
        "# Create a confusion matrix on training data.\n",
        "with tf.Graph().as_default():\n",
        "  cm = tf.confusion_matrix(train_df[\"polarity\"], \n",
        "                           get_predictions(estimator, predict_train_input_fn))\n",
        "  with tf.Session() as session:\n",
        "    cm_out = session.run(cm)\n",
        "\n",
        "# Normalize the confusion matrix so that each row sums to 1.\n",
        "cm_out = cm_out.astype(float) / cm_out.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "sns.heatmap(cm_out, annot=True, xticklabels=LABELS, yticklabels=LABELS);\n",
        "plt.xlabel(\"Predicted\");\n",
        "plt.ylabel(\"True\");"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXIAAAEKCAYAAAAPVd6lAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nwAA/AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "sG-ES55Ftp-t"
      },
      "source": [
        "# Further improvements\n",
        "\n",
        "1. **Regression on sentiment**: we used a classifier to assign each example into a polarity class. But we actually have another categorical feature at our disposal - sentiment. Here classes actually represent a scale and the underlying value (positive/negative) could be well mapped into a continuous range. We could make use of this property by computing a regression ([DNN Regressor](https://www.tensorflow.org/api_docs/python/tf/contrib/learn/DNNRegressor)) instead of a classification ([DNN Classifier](https://www.tensorflow.org/api_docs/python/tf/contrib/learn/DNNClassifier)).\n",
        "2. **Larger module**: for the purposes of this tutorial we used a small module to restrict the memory use. There are modules with larger vocabularies and larger embedding space that could give additional accuracy points.\n",
        "3. **Parameter tuning**: we can improve the accuracy by tuning the meta-parameters like the learning rate or the number of steps, especially if we use a different module. A validation set is very important if we want to get any reasonable results, because it is very easy to set-up a model that learns to predict the training data without generalizing well to the test set.\n",
        "4. **More complex model**: we used a module that computes a sentence embedding by embedding each individual word and then combining them with average. One could also use a sequential module (e.g. [Universal Sentence Encoder](https://tfhub.dev/google/universal-sentence-encoder/2) module) to better capture the nature of sentences. Or an ensemble of two or more TF-Hub modules.\n",
        "5. **Regularization**: to prevent overfitting, we could try to use an optimizer that does some sort of regularization, for example [Proximal Adagrad Optimizer](https://www.tensorflow.org/api_docs/python/tf/train/ProximalAdagradOptimizer).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fKRNsaO8L50F"
      },
      "source": [
        "# Advanced: Transfer learning analysis\n",
        "\n",
        "Transfer learning makes it possible to **save training resources** and to achieve good model generalization even when **training on a small dataset**. In this part, we will demonstrate this by training with two different TF-Hub modules:\n",
        "\n",
        "* **[nnlm-en-dim128](https://tfhub.dev/google/nnlm-en-dim128/1)** - pretrained text embedding module,\n",
        "* **[random-nnlm-en-dim128](https://tfhub.dev/google/random-nnlm-en-dim128/1)** - text embedding module that has same vocabulary and network as **nnlm-en-dim128**, but the weights were just randomly initialized and never trained on real data.\n",
        "\n",
        "And by training in two modes: \n",
        "\n",
        "* training **only the classifier** (i.e. freezing the module), and \n",
        "* training the **classifier together with the module**.\n",
        "\n",
        "Let's run a couple of trainings and evaluations to see how using a various modules can affect the accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "AWYa1So1ARyz",
        "outputId": "4e880921-f9ab-4db3-9480-477808f881f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400
        }
      },
      "source": [
        "def train_and_evaluate_with_module(hub_module, train_module=False):\n",
        "  embedded_text_feature_column = hub.text_embedding_column(\n",
        "      key=\"sentence\", module_spec=hub_module, trainable=train_module)\n",
        "\n",
        "  estimator = tf.estimator.DNNClassifier(\n",
        "      hidden_units=[500, 100],\n",
        "      feature_columns=[embedded_text_feature_column],\n",
        "      n_classes=2,\n",
        "      optimizer=tf.train.AdagradOptimizer(learning_rate=0.003))\n",
        "\n",
        "  estimator.train(input_fn=train_input_fn, steps=1000)\n",
        "\n",
        "  train_eval_result = estimator.evaluate(input_fn=predict_train_input_fn)\n",
        "  test_eval_result = estimator.evaluate(input_fn=predict_test_input_fn)\n",
        "\n",
        "  training_set_accuracy = train_eval_result[\"accuracy\"]\n",
        "  test_set_accuracy = test_eval_result[\"accuracy\"]\n",
        "\n",
        "  return {\n",
        "      \"Training accuracy\": training_set_accuracy,\n",
        "      \"Test accuracy\": test_set_accuracy\n",
        "  }\n",
        "\n",
        "\n",
        "results = {}\n",
        "results[\"nnlm-en-dim128\"] = train_and_evaluate_with_module(\n",
        "    \"https://tfhub.dev/google/nnlm-en-dim128/1\")\n",
        "results[\"nnlm-en-dim128-with-module-training\"] = train_and_evaluate_with_module(\n",
        "    \"https://tfhub.dev/google/nnlm-en-dim128/1\", True)\n",
        "results[\"random-nnlm-en-dim128\"] = train_and_evaluate_with_module(\n",
        "    \"https://tfhub.dev/google/random-nnlm-en-dim128/1\")\n",
        "results[\"random-nnlm-en-dim128-with-module-training\"] = train_and_evaluate_with_module(\n",
        "    \"https://tfhub.dev/google/random-nnlm-en-dim128/1\", True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-358adc88f2cd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m     \"https://tfhub.dev/google/nnlm-en-dim128/1\", True)\n\u001b[1;32m     30\u001b[0m results[\"random-nnlm-en-dim128\"] = train_and_evaluate_with_module(\n\u001b[0;32m---> 31\u001b[0;31m     \"https://tfhub.dev/google/random-nnlm-en-dim128/1\")\n\u001b[0m\u001b[1;32m     32\u001b[0m results[\"random-nnlm-en-dim128-with-module-training\"] = train_and_evaluate_with_module(\n\u001b[1;32m     33\u001b[0m     \"https://tfhub.dev/google/random-nnlm-en-dim128/1\", True)\n",
            "\u001b[0;32m<ipython-input-9-358adc88f2cd>\u001b[0m in \u001b[0;36mtrain_and_evaluate_with_module\u001b[0;34m(hub_module, train_module)\u001b[0m\n\u001b[1;32m      9\u001b[0m       optimizer=tf.train.AdagradOptimizer(learning_rate=0.003))\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m   \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_input_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m   \u001b[0mtrain_eval_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpredict_train_input_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, input_fn, hooks, steps, max_steps, saving_listeners)\u001b[0m\n\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m       \u001b[0msaving_listeners\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_listeners_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 367\u001b[0;31m       \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    368\u001b[0m       \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loss for final step: %s.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_train_model\u001b[0;34m(self, input_fn, hooks, saving_listeners)\u001b[0m\n\u001b[1;32m   1156\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model_distributed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1157\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1158\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1160\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_train_model_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_train_model_default\u001b[0;34m(self, input_fn, hooks, saving_listeners)\u001b[0m\n\u001b[1;32m   1190\u001b[0m       return self._train_with_estimator_spec(estimator_spec, worker_hooks,\n\u001b[1;32m   1191\u001b[0m                                              \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step_tensor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1192\u001b[0;31m                                              saving_listeners)\n\u001b[0m\u001b[1;32m   1193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1194\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_train_model_distributed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_train_with_estimator_spec\u001b[0;34m(self, estimator_spec, worker_hooks, hooks, global_step_tensor, saving_listeners)\u001b[0m\n\u001b[1;32m   1482\u001b[0m       \u001b[0many_step_done\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1483\u001b[0m       \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmon_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1484\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmon_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mestimator_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestimator_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1485\u001b[0m         \u001b[0many_step_done\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1486\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0many_step_done\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    752\u001b[0m         \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 754\u001b[0;31m         run_metadata=run_metadata)\n\u001b[0m\u001b[1;32m    755\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrun_step_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1250\u001b[0m             \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1251\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1252\u001b[0;31m             run_metadata=run_metadata)\n\u001b[0m\u001b[1;32m   1253\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0m_PREEMPTION_ERRORS\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1254\u001b[0m         logging.info(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1336\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1337\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1338\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1339\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_PREEMPTION_ERRORS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1340\u001b[0m       \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1409\u001b[0m         \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1410\u001b[0m         \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1411\u001b[0;31m         run_metadata=run_metadata)\n\u001b[0m\u001b[1;32m   1412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1413\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1168\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1169\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1171\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrun_step_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_with_hooks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 950\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    951\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1171\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1172\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1173\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1174\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1350\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1354\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1355\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1356\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1357\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1339\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1340\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1341\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1427\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1428\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1429\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1431\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "CsWppYMphIPh"
      },
      "source": [
        "Let's look at the results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UVkdErEKkIXL",
        "colab": {}
      },
      "source": [
        "pd.DataFrame.from_dict(results, orient=\"index\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Z9rZ2fuGfUFh"
      },
      "source": [
        "We can already see some patterns, but first we should establish the baseline accuracy of the test set - the lower bound that can be achieved by outputting only the label of the most represented class:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IgYPVvc3G6OS",
        "colab": {}
      },
      "source": [
        "estimator.evaluate(input_fn=predict_test_input_fn)[\"accuracy_baseline\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "UN4D-DPPrINX"
      },
      "source": [
        "Assigning the most represented class will give us accuracy of **50%**. There are a couple of things to notice here:\n",
        "\n",
        "1. Maybe surprisingly, **a model can still be learned on top of fixed, random embeddings**. The reason is that even if every word in the dictionary is mapped to a random vector, the estimator can separate the space purely using its fully connected layers.\n",
        "2. Allowing training of the module with **random embeddings** increases both training and test accuracy as oposed to training just the classifier.\n",
        "3. Training of the module with **pre-trained embeddings** also increases both accuracies. Note however the overfitting on the training set. Training a pre-trained module can be dangerous even with regularization in the sense that the embedding weights no longer represent the language model trained on diverse data, instead they converge to the ideal representation of the new dataset."
      ]
    }
  ]
}